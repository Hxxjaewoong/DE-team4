import json
import boto3
import pandas as pd
import io
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# AWS í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
s3 = boto3.client('s3')
lambda_client = boto3.client('lambda')

# í™˜ê²½ ë³€ìˆ˜
BUCKET_NAME = "hmg5th-4-bucket"
RAW_HTML_PREFIX = "raw_html/fmkorea/"
PROCESSED_DATA_PREFIX = "raw_data/fmkorea/"
LOGGING_LAMBDA_ARN = "arn:aws:lambda:ap-northeast-2:473551908409:function:crawling_log_lambda"

def extract_content(html, url, keyword):
    """HTMLì—ì„œ ë³¸ë¬¸ ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # ë‚ ì§œ
        date_element = soup.select_one("span.date.m_no")
        post_date = date_element.get_text(strip=True).split("ìˆ˜ì •ì¼")[0].strip() if date_element else ""
        post_date = datetime.strptime(post_date, "%Y.%m.%d %H:%M")

        # ì œëª©
        title_element = soup.select_one("h1 span.np_18px_span")
        title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"

        # ë³¸ë¬¸ ë‚´ìš©
        content_element = soup.select_one(".xe_content")
        content = content_element.get_text(strip=True) if content_element else ""

        # ì‘ì„±ì
        author_element = soup.select_one(".member_plate")
        author = author_element.get_text(strip=True) if author_element else "ì•Œ ìˆ˜ ì—†ìŒ"

        # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜
        stats = soup.select(".btm_area .fr span b")
        views = stats[0].get_text(strip=True) if len(stats) > 0 else "0"
        likes = stats[1].get_text(strip=True) if len(stats) > 1 else "0"
        hates = "0" # í¨ì½”ëŠ” ì‹«ì–´ìš”ê°€ ì—†ìŒ?

        # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
        comments = extract_comments(soup, url, title)
        comments_count = len(comments)

        # ë³¸ë¬¸ì´ ìˆëŠ”ë° íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš° ë¡œê·¸ ì „ì†¡
        if content_element and not content:
            log_error("extract_content", url, "ë³¸ë¬¸ì´ ì¡´ì¬í•˜ì§€ë§Œ íŒŒì‹± ì‹¤íŒ¨")

        return {
            "site": "fmkorea",
            "datetime": post_date,
            "model": keyword,
            "title": title,
            "content": content,
            "url": url,
            "author": author,
            "likes": likes,
            "hates": hates,
            "comments_count": comments_count,
            "views": views
        }
    except Exception as e:
        log_error("extract_content", url, f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None  # ğŸ”´ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

def extract_comments(soup, url, title):
    """HTMLì—ì„œ **ëª¨ë“  ëŒ“ê¸€**ì„ ì¶”ì¶œ"""
    comments = []
    try:
        comments_ul = soup.select_one("ul.fdb_lst_ul")
        comments_locator = comments_ul.select("div.comment-content") if comments_ul else []

        if not comments_locator:
            print(f"âš ï¸ ëŒ“ê¸€ì´ ì—†ëŠ” ê¸€: {url}")
            return comments  # ëŒ“ê¸€ ì—†ìŒ

        for comment in comments_locator:
            text = comment.get_text(strip=True)
            if text:
                comments.append({
                    "url": url,
                    "title": title,
                    "comment": text
                })

        if not comments:
            log_error("extract_comments", url, "ëŒ“ê¸€ì´ ì¡´ì¬í•˜ì§€ë§Œ íŒŒì‹± ì‹¤íŒ¨")
            print(f"âŒ ëŒ“ê¸€ì´ ìˆëŠ” ê¸€ì´ì§€ë§Œ íŒŒì‹± ì‹¤íŒ¨: {url}")

    except Exception as e:
        log_error("extract_comments", url, f"ëŒ“ê¸€ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        print(f"âŒ ëŒ“ê¸€ íŒŒì‹± ì˜¤ë¥˜: {e}")

    return comments

def log_error(stage, url, error_message):
    """Lambdaë¡œ ì—ëŸ¬ ë¡œê¹… ì „ì†¡"""
    log_payload = {
        "status": "error",
        "source": "fmkorea_parse",
        "stage": stage,
        "url": url,
        "error": error_message
    }
    lambda_client.invoke(
        FunctionName=LOGGING_LAMBDA_ARN,
        InvocationType="Event",
        Payload=json.dumps(log_payload)
    )

def lambda_handler(event, context):
    """S3ì—ì„œ HTML íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ íŒŒì‹± í›„ CSVë¡œ ì €ì¥"""
    
    today_date = datetime.utcnow().strftime('%Y-%m-%d')
    #today_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d")
    s3_key = f"{RAW_HTML_PREFIX}{today_date}.json"

    try:
        response = s3.get_object(Bucket=BUCKET_NAME, Key=s3_key)
        html_data = json.loads(response['Body'].read().decode('utf-8-sig'))
    except s3.exceptions.NoSuchKey:
        error_msg = f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {s3_key}"
        print(f"âŒ {error_msg}")
        log_error("load_html", s3_key, error_msg)
        return {"status": "NoSuchKey", "key": s3_key}
    except json.JSONDecodeError as e:
        error_msg = f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        log_error("load_html", s3_key, error_msg) 
        return {"status": "JSONDecodeError", "error": str(e)}
    except Exception as e:
        error_msg = f"S3ì—ì„œ HTML ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        log_error("load_html", s3_key, error_msg)  
        return {"status": "Failed to load HTML data", "error": str(e)}

    content_data = []
    comment_data = []

    for url, data in html_data.items():
        keyword = data["keyword"]
        html = data["html"]

        # ë³¸ë¬¸ ë°ì´í„° ì¶”ì¶œ (Noneì´ ì•„ë‹ˆë©´ ì¶”ê°€)
        content = extract_content(html, url, keyword)
        if content:
            content_data.append(content)

            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ í›„ ì¶”ê°€
            comments = extract_comments(BeautifulSoup(html, "html.parser"), url, content["title"])
            comment_data.extend(comments)

    content_file_key = None
    comment_file_key = None

    # ë³¸ë¬¸ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥
    if content_data:
        content_df = pd.DataFrame(content_data)
        content_buffer = io.BytesIO()
        content_df.to_parquet(content_buffer, index=False)

        content_file_key = f"{PROCESSED_DATA_PREFIX}{today_date}-content.parquet"
        s3.put_object(Bucket=BUCKET_NAME, Key=content_file_key, Body=content_buffer.getvalue(), ContentType="application/octet-stream")
        print(f"âœ… ë³¸ë¬¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {content_file_key}")

    # ëŒ“ê¸€ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥
    if comment_data:
        comment_df = pd.DataFrame(comment_data)
        comment_buffer = io.BytesIO()
        comment_df.to_parquet(comment_buffer, index=False)

        comment_file_key = f"{PROCESSED_DATA_PREFIX}{today_date}-comment.parquet"
        s3.put_object(Bucket=BUCKET_NAME, Key=comment_file_key, Body=comment_buffer.getvalue(), ContentType="application/octet-stream")
        print(f"âœ… ëŒ“ê¸€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {comment_file_key}")

    return {
        "status": "Processing completed",
        "content_file": content_file_key if content_file_key else "No content data",
        "comment_file": comment_file_key if comment_file_key else "No comment data"
    }
