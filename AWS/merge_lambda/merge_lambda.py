import json
import boto3
import pandas as pd
import io
import requests
import os
from datetime import datetime, timedelta

# AWS í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
s3 = boto3.client("s3")
lambda_client = boto3.client("lambda")

# í™˜ê²½ ë³€ìˆ˜
BUCKET_NAME = os.getenv("BUCKET_NAME", "hmg5th-4-bucket")
RAW_DATA_PREFIX = "raw_data/"
MERGED_CONTENT_PREFIX = "merge_data/contents/"
MERGED_COMMENT_PREFIX = "merge_data/comments/"
LOGGING_LAMBDA_ARN = os.getenv("LOGGING_LAMBDA_ARN", "arn:aws:lambda:ap-northeast-2:473551908409:function:crawling_log_lambda")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Slack Webhook URL ê°€ì ¸ì˜¤ê¸°

def list_s3_files(prefix):
    """ì£¼ì–´ì§„ prefixë¡œ S3ì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=prefix)
        return [obj["Key"] for obj in response.get("Contents", [])]
    except Exception as e:
        log_error("list_s3_files", f"S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return []

def load_csv_from_s3(file_key):
    """S3ì—ì„œ CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë³€í™˜"""
    try:
        response = s3.get_object(Bucket=BUCKET_NAME, Key=file_key)
        return pd.read_csv(io.BytesIO(response["Body"].read()), encoding="utf-8-sig")
    except Exception as e:
        log_error("load_csv_from_s3", f"S3 CSV ë¡œë“œ ì‹¤íŒ¨ ({file_key}): {str(e)}")
        return None

def upload_csv_to_s3(df, file_key):
    """ë³‘í•©ëœ DataFrameì„ S3ì— CSVë¡œ ì €ì¥"""
    try:
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
        s3.put_object(Bucket=BUCKET_NAME, Key=file_key, Body=csv_buffer.getvalue().encode("utf-8-sig"), ContentType="text/csv; charset=utf-8")
        print(f"âœ… ë³‘í•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_key}")
    except Exception as e:
        log_error("upload_csv_to_s3", f"ë³‘í•© ë°ì´í„° S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({file_key}): {str(e)}")


def upload_parquet_to_s3(df, file_key):
    """ë³‘í•©ëœ DataFrameì„ S3ì— Parquet íŒŒì¼ë¡œ ì €ì¥"""
    try:
        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False)
        s3.put_object(Bucket=BUCKET_NAME, Key=file_key, Body=buffer.getvalue(), ContentType="application/octet-stream")
        print(f"âœ… ë³‘í•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_key}")
    except Exception as e:
        log_error("upload_parquet_to_s3", f"ë³‘í•© ë°ì´í„° S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({file_key}): {str(e)}")



def merge_files(file_keys):
    """S3ì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë³‘í•©"""
    dataframes = [load_csv_from_s3(file_key) for file_key in file_keys if load_csv_from_s3(file_key) is not None]
    
    if not dataframes:
        print("âš ï¸ ë³‘í•©í•  ë°ì´í„° ì—†ìŒ")
        return None

    return pd.concat(dataframes, ignore_index=True)

def log_error(stage, error_message):
    """Lambdaë¡œ ì—ëŸ¬ ë¡œê¹… ì „ì†¡"""
    log_payload = {
        "status": "error",
        "source": "merge_lambda",
        "stage": stage,
        "error": error_message,
        "timestamp": datetime.utcnow().isoformat()
    }
    try:
        lambda_client.invoke(
            FunctionName=LOGGING_LAMBDA_ARN,
            InvocationType="Event",
            Payload=json.dumps(log_payload)
        )
        print(f"ğŸ“Œ ì˜¤ë¥˜ ê¸°ë¡: {log_payload}")
    except Exception as e:
        print(f"âŒ ë¡œê·¸ ëŒë‹¤ í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


def lambda_handler(event, context):
    """S3ì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œì˜ ë³¸ë¬¸ & ëŒ“ê¸€ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë³‘í•© í›„ ì €ì¥"""
    
    today_date = datetime.utcnow().strftime("%Y-%m-%d")
    #today_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d")

    # 1ï¸âƒ£ ì˜¤ëŠ˜ ë‚ ì§œì˜ content.csv & comment.csv íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    content_files = list_s3_files(f"{RAW_DATA_PREFIX}fmkorea/{today_date}-content.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}dcinside/{today_date}-content.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}clien/{today_date}-content.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}bobae/{today_date}-content.csv")

    comment_files = list_s3_files(f"{RAW_DATA_PREFIX}fmkorea/{today_date}-comment.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}dcinside/{today_date}-comment.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}clien/{today_date}-comment.csv") + \
                    list_s3_files(f"{RAW_DATA_PREFIX}bobae/{today_date}-comment.csv")

    # 2ï¸âƒ£ ë³¸ë¬¸ ë°ì´í„° ë³‘í•©
    merged_content_df = merge_files(content_files)
    if merged_content_df is not None:
        merged_content_key = f"{MERGED_CONTENT_PREFIX}{today_date}.parquet"
        upload_parquet_to_s3(merged_content_df, merged_content_key)
    else:
        log_error("lambda_handler", "ë³¸ë¬¸ ë°ì´í„° ì—†ìŒ: content_files ë¹„ì–´ ìˆìŒ")


    # 3ï¸âƒ£ ëŒ“ê¸€ ë°ì´í„° ë³‘í•©
    merged_comment_df = merge_files(comment_files)
    if merged_comment_df is not None:
        merged_comment_key = f"{MERGED_COMMENT_PREFIX}{today_date}.parquet"
        upload_parquet_to_s3(merged_comment_df, merged_comment_key)
    else:
        log_error("lambda_handler", "ëŒ“ê¸€ ë°ì´í„° ì—†ìŒ: comment_files ë¹„ì–´ ìˆìŒ")


    # 4ï¸âƒ£ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ Step Functionì— ì˜ˆì™¸ ì „ë‹¬ ë° Slack ì•Œë¦¼
    if merged_content_df is None and merged_comment_df is None:
        error_message = "ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ì–´ Step Functionì—ì„œ ë³‘í•© ì‹¤íŒ¨ë¡œ ì²˜ë¦¬."
        log_error("lambda_handler", error_message)
        raise Exception("NoDataToMergeException: ë³‘í•©í•  ë°ì´í„° ì—†ìŒ.")

    return {
        "status": "Merge process completed",
        "merged_content_file": merged_content_key if merged_content_key else "No content data",
        "merged_comment_file": merged_comment_key if merged_comment_key else "No comment data"
    }