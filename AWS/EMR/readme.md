# ⚡ EMR - Spark Job Optimization

## 🚀 코드 레벨 최적화 (Code-Level Optimization)

Spark Job의 성능을 최적화하기 위해 아래와 같은 개선 작업을 수행하였습니다.

### 📌 최적화 항목

1️⃣ **데이터 포맷 변경**
   - 기존 CSV 대신 Parquet 형식 사용 → **입출력 성능 향상**

2️⃣ **Iterator를 사용한 데이터프레임 순회 제거**
   - 데이터프레임을 직접 조작하는 코드 제거
   - Spark의 **내장 API 활용**하여 병렬 처리 최적화

3️⃣ **감정 분석 최적화**
   - 기존 **Lambda + Reduce 방식 제거**
   - **정규식 + UDF 적용**하여 속도 개선

4️⃣ **컬럼 선택 최적화**
   - `drop` 대신 **`select` 사용** → 불필요한 데이터 로딩 방지

5️⃣ **반복 사용되는 데이터 캐싱**
   - `df_merged`, `df_keywords`를 **cache()로 캐싱**하여 재사용 시 성능 개선

6️⃣ **Broadcast 활용**
   - 상대적으로 작은 크기의 **카테고리 분류용 데이터**와 **감정 분석용 테이블**을 **Broadcast 변수로 설정** → Join 성능 개선

7️⃣ **컬럼 가지치기 (Pruning)**
   - 더 이상 참조하지 않는 컬럼을 **미리 제거**하여 메모리 사용량 절감

---

## 📉 성능 개선 결과

📊 **동일 데이터셋 기준** 실행 시간 **최대 37초 → 16초**까지 단축!

🔹 **데이터 로딩 최적화**로 Spark Job의 **I/O 비용 절감**
🔹 **연산 병렬화**를 통해 **CPU 자원 활용 극대화**
🔹 **캐싱 및 Broadcast 적용**으로 **반복 연산 비용 감소**

---

최적화된 Spark Job을 통해 더 빠르고 효율적인 데이터 처리가 가능해졌습니다! 🚀

